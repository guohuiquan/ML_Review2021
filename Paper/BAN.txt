BAN 双线性注意力机制
	考虑两组输入通道之间的双线性交互作用，低级双线池化则是提取每一对通道的联合表示。与co-attention不同，co-attention构建两个单独的注意力分布，为避免计算的复杂性，忽略了多模式输入之间的相互作用。
	应用：variant of 多模态残差网络
		能够高效利用ban的八个attention map，能够在模型定义的图像空间地图中选择性学习。集成来自多个双线性注意图的联合表示
low-rank 双线性池化
	该网络使用单通道输入（问题向量）将其他多通道输入（图像特征）组合为单通道的中间表示。	
单一注意力网络
	通过选择性的利用给定的信息来减少输入通道。假设多通道输入Y由「Yi」组成，单一通道权重设置，权重表示有选择地组合的注意力分布
	双线性注意力网络
		引入双线性关注图来同时减少两个多输入通道
Co-Attention
	将共同关注方法分为两个步骤，即问题嵌入的自我关注和视觉嵌入的提问定向关注。但是，这些共同关注方法对每个模式使用单独的注意力分布，忽略了模型之间的相互作用。
------------------------------------------------------------------------